# -*- coding: utf-8 -*-
"""pp2-entrega2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zxnMZ6Z75CNz7q9AFxbVDJ115LvOBGHf
"""

# ðŸ”„ Downgrade a versiones compatibles y reinicia
!pip install numpy==1.24.4 pandas==1.5.3 ydata-profiling==4.6.2 --quiet
import os; os.kill(os.getpid(), 9)

# SelecciÃ³n de librerias para trabajar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el dataset seleccionado
df = pd.read_csv('healthcare-dataset-stroke-data.csv')

# Primera exploraciÃ³n
print("Primeras filas del dataset:")
display(df.head())

print("\nInformaciÃ³n general:")
print(df.info())

print("\nValores nulos por columna:")
print(df.isnull().sum())

"""## EDA

"""

# Resumen estadÃ­stico general
print(df.describe(include='all'))

# Revisar valores nulos por columna
print(df.isnull().sum())

# Crear tabla de proporciones
proportions = df.groupby('hypertension')['stroke'].value_counts(normalize=True).unstack()

# GrÃ¡fico de barras apiladas
proportions.plot(kind='bar', stacked=True, figsize=(8, 4), colormap='coolwarm')
plt.title('ProporciÃ³n de Stroke segÃºn HipertensiÃ³n')
plt.xlabel('HipertensiÃ³n (0 = No, 1 = SÃ­)')
plt.ylabel('ProporciÃ³n')
plt.legend(title='Stroke', labels=['No', 'SÃ­'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Histograma de edades
plt.figure(figsize=(13, 4))
sns.histplot(df['age'], kde=True, bins=30)
plt.title('DistribuciÃ³n de Edad')
plt.xlabel('Edad')
plt.ylabel('Frecuencia')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Crear categorÃ­as de edad cada 10 aÃ±os
df['age_group'] = pd.cut(df['age'], bins=range(0, 101, 10), right=False, include_lowest=True)

# Ordenar categorÃ­as para que aparezcan en orden en el grÃ¡fico
df['age_group'] = df['age_group'].astype(str)

# Boxplot de glucosa por grupo de edad
plt.figure(figsize=(17, 4))
sns.boxplot(x='age_group', y='avg_glucose_level', data=df)
plt.title('Nivel de Glucosa por Grupo de Edad (cada 10 aÃ±os)')
plt.xlabel('Grupo de Edad')
plt.ylabel('Nivel promedio de glucosa')
plt.xticks(rotation=45)
plt.show()

"""### EDA AUTOMATIZADO CON PROFILING"""

from ydata_profiling import ProfileReport
profile = ProfileReport(df, title="EDA completo", explorative=True)
profile.to_notebook_iframe()

"""##AnÃ¡lisis Ã‰tico (DetecciÃ³n de Sesgos)

1. MÃ©tricas de Equidad: Chi-Cuadrado entre stroke y gender
"""

from scipy.stats import chi2_contingency

contingencia = pd.crosstab(df['gender'], df['stroke'])
chi2, p, dof, expected = chi2_contingency(contingencia)

print(f"Chi-cuadrado: {chi2}")
print(f"p-valor: {p}")

"""2. Fairlearn Dashboard (visualizaciÃ³n de disparidades)

    DistribuciÃ³n de la variable objetivo por grupo.

    Disparidades en tasas de aprobaciÃ³n/rechazo.

    ComparaciÃ³n de mÃ©tricas bÃ¡sicas (ej.: proporciones).

estamos usando esta version para COLAB, parajupyter notebook es un deshboard con vizualisaciones
"""

!pip install fairlearn --quiet

from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split # Import train_test_split
from sklearn.linear_model import LogisticRegression # Import LogisticRegression

# Entrenamiento del modelo
# Usamos 'df' en lugar de 'df_clean' ya que 'df' es el dataframe que hemos limpiado.
X = df[['age', 'avg_glucose_level', 'bmi']]
y = df['stroke']
sensitive_feature = df['gender']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

model = LogisticRegression(solver='liblinear') # Especificamos un solver para evitar warnings
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Aseguramos que sensitive_features coincida con los Ã­ndices de y_test
sensitive_feature_test = sensitive_feature.loc[y_test.index]

# Crear mÃ©tricas con fairlearn
metric_frame = MetricFrame(
    metrics={"accuracy": accuracy_score, "selection_rate": selection_rate},
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=sensitive_feature_test
)

print("MÃ©tricas por grupo sensible (gÃ©nero):")
print(metric_frame.by_group)

# Diferencias de equidad
dp_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive_feature_test)
eo_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive_feature_test)

print(f"\nDiferencia de paridad demogrÃ¡fica: {dp_diff:.4f}")
print(f"Diferencia de igualdad de oportunidades: {eo_diff:.4f}")

"""IdentificaciÃ³n de variables Proxy"""

# Ver primeras filas (pueden sugerir proxies como work_type, residence_type)
print(df.head())

# Buscar correlaciones de categÃ³ricas con la variable target
for col in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:
    print(f"\nDistribuciÃ³n de stroke por: {col}")
    print(df.groupby(col)['stroke'].mean())

"""##Limpieza de Datos
1.1. Detectar y manejar valores faltantes (NaN)
"""

#Eliminamos valores nulos
df = df.dropna()

# Eliminamos 'id' porque no aporta al modelo
df = df.drop(columns=['id'], errors='ignore') #Usamos errors='ignore' para evitar un KeyError si la columna 'id' no existe

# Ver cuÃ¡ntos valores faltantes hay por columna
print("Valores faltantes por columna:")
print(df.isnull().sum())

# eliminar filas con valores faltantes
df_clean = df.dropna()
print(f"Filas despuÃ©s de eliminar nulos: {df_clean.shape[0]}")

#o imputar con la media:
#df['bmi'] = df['bmi'].fillna(df['bmi'].mean())

"""1.2. Detectar outliers (valores extremos)
Podemos usar el mÃ©todo IQR (rango intercuartÃ­lico) para detectar y eliminar outliers en columnas como age, avg_glucose_level, bmi:
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Copia del original para comparaciÃ³n
df_original = df.copy()

# FunciÃ³n para quitar outliers por IQR
def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return df[(df[column] >= lower) & (df[column] <= upper)]

# Aplicamos el filtro de outliers
df_clean = df.copy()
for col in ['age', 'avg_glucose_level', 'bmi']:
    df_clean = remove_outliers_iqr(df_clean, col)

# FunciÃ³n para graficar antes y despuÃ©s
def boxplot_comparison(before, after, column):
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    sns.boxplot(y=before[column], ax=axes[0], color="tomato")
    axes[0].set_title(f"{column} - Antes (con outliers)")
    sns.boxplot(y=after[column], ax=axes[1], color="seagreen")
    axes[1].set_title(f"{column} - DespuÃ©s (sin outliers)")
    plt.tight_layout()
    plt.show()

# Mostramos los grÃ¡ficos
for col in ['age', 'avg_glucose_level', 'bmi']:
    boxplot_comparison(df_original, df_clean, col)

"""1.3. Corregir errores tipogrÃ¡ficos en variables categÃ³ricas
Ejemplo con la columna gender:
"""

# Ver valores Ãºnicos en la columna
print("Valores Ãºnicos de 'gender':", df_clean['gender'].unique())

# Normalizar errores comunes
df_clean['gender'] = df_clean['gender'].replace({
    'Femenno': 'Femenino',
    'Femmenino': 'Femenino',
    'Mascuulino': 'Masculino',
    'Masculno': 'Masculino'
})

#df_clean['gender'] = df_clean['gender'].str.lower().str.capitalize() (para hacer todo minuscula)

"""Paso 2: Codificar Variables

Â¿QuÃ© es Codificar?

Transformar datos no numÃ©ricos en nÃºmeros

Ejemplo:

GÃ©nero: "Femenino" â†’ 0, "Masculino" â†’ 1, "No binario" â†’ 2.

Evita codificar variables sensibles si no es necesario (podrÃ­an causar sesgos).

## âš–ï¸ PASO 3 TÃ©cnicas para MitigaciÃ³n de Sesgos
Paso 3.1: Verificamos el desbalance
"""

print(df_clean['gender'].value_counts(normalize=True))
print(df_clean['work_type'].value_counts(normalize=True))

"""Paso 3.2: Balanceo de datos


1.   Sobremuestreo del grupo minoritario (SMOTE, RandomOverSampler)  : Agregar mÃ¡s datos de grupos minoritarios.

2.   Submuestreo del grupo mayoritario (RandomUnderSampler) Reducir datos de grupos mayoritarios.

## Paso 4: Feature Engineering General

Transformaciones MatemÃ¡ticas:

NormalizaciÃ³n: Escalar variables a un rango comÃºn (ej.: 0 a 1).
Logaritmos: Para manejar valores extremos.

| TransformaciÃ³n                           | CuÃ¡ndo usarla                                                                                                                                    | LibrerÃ­a                                           |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------- |
| **MinMaxScaler (0 a 1)**                 | Cuando querÃ©s preservar la forma de la distribuciÃ³n pero llevar todo al mismo rango. Ideal para redes neuronales.                                | `from sklearn.preprocessing import MinMaxScaler`   |
| **StandardScaler (media 0, varianza 1)** | Cuando los datos tienen una distribuciÃ³n normal o simÃ©trica. ComÃºn en regresiÃ³n logÃ­stica o SVM.                                                 | `from sklearn.preprocessing import StandardScaler` |
| **Log (np.log1p)**                       | Cuando hay valores **muy grandes** o sesgo a la derecha (distribuciÃ³n sesgada). Muy Ãºtil para `'avg_glucose_level'`, `'bmi'` si tienen outliers. | `import numpy as np`                               |
"""

#Antes de normalizar o estandarizar veremos si hay sesgos en los datos
import numpy as np

df.describe()

import seaborn as sns
import matplotlib.pyplot as plt

num_vars = ['avg_glucose_level', 'age', 'bmi']  # Solo variables numÃ©ricas

for var in num_vars:
    plt.figure(figsize=(2,2))
    sns.histplot(df_clean[var], kde=True, bins=30)
    plt.title(f"DistribuciÃ³n de {var}")
    plt.show()

"""**Resumen de los sesgos de los datos**:

Usa log-transform para avg_glucose_level porque tiene sesgo positivo fuerte.

Prueba normalizar o estandarizar age y bmi para que las variables estÃ©n en rangos comparables.

No transformes las variables binarias (hypertension, heart_disease, stroke).

Segun el tipo de modelo que usaremos y segun la variable que usaremos:

Algunos modelos (como regresiÃ³n logÃ­stica, SVM o redes neuronales) se benefician mucho si las variables numÃ©ricas estÃ¡n escaladas (normalizadas o estandarizadas).

Otros modelos como Ã¡rboles de decisiÃ³n o random forests no necesitan transformaciones tan estrictas porque no se basan en distancias ni en escala de variables.

#Modelado con Random Forest
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import LabelEncoder

# Variables seleccionadas
features = ['age', 'bmi', 'hypertension', 'heart_disease',
            'gender', 'smoking_status', 'Residence_type']
target = 'stroke'

# Eliminar filas con valores nulos en variables seleccionadas
df = df[features + [target]].dropna()

# Codificar variables categÃ³ricas
cat_vars = ['gender', 'smoking_status', 'Residence_type']
df_encoded = pd.get_dummies(df, columns=cat_vars, drop_first=True)

# Separar variables X e y
X = df_encoded.drop(columns=[target])
y = df_encoded[target]

# Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar modelo Random Forest
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predicciones
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# EvaluaciÃ³n
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_prob))

# Importancia de variables
importances = pd.Series(model.feature_importances_, index=X.columns)
importances.sort_values(ascending=False).plot(kind='bar', title='Importancia de Variables')

"""SegÃºn el grÃ¡fico de importancia, Estas dos variables dominan el modelo, con mucha mÃ¡s importancia que el resto.



bmi (Ã­ndice de masa corporal)

age (edad)

Rendimiento del modelo:
PrecisiÃ³n global (accuracy): 94%

PrecisiÃ³n en clase 0 (sin stroke): 95%

PrecisiÃ³n en clase 1 (con stroke): 20%

Recall en clase 1: 2% âŒ

Solo detectÃ³ 1 de 53 strokes verdaderos.

ðŸ”Ž Problema identificado:
Hay un fuerte desbalance de clases:

929 casos sin stroke

53 con stroke

El modelo aprende a predecir la clase mayoritaria y apenas detecta los casos positivos, que son los mÃ¡s importantes.

##ConclusiÃ³n breve
El modelo Random Forest logrÃ³ una alta precisiÃ³n global (94%), pero fallÃ³ al predecir casos positivos de stroke (recall = 2%), debido al fuerte desbalance de clases. Las variables mÃ¡s influyentes fueron bmi y age. Se recomienda abordar el desbalance aplicando tÃ©cnicas como class_weight, SMOTE u otros mÃ©todos de resampling para mejorar la detecciÃ³n de casos positivos.
Tambien podriamos probar con otros modelos como:

XGBoost (mayor precisiÃ³n en desbalance)

Logistic Regression con regularizaciÃ³n

Modelos de boosting con class_weight o focal loss
"""